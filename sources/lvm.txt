
Upgrade safety net with the logical volume manager
==================================================

The story is a website with a database which gets into production and
then, after a while, needs to have its schema corrected and related
files upgraded safely. As usual, it is always better to get the schema
right from the beginning but it the real world there is a demand for
both flexibility and safety. With the logical volume manager, the
website gets its invasive (read scary) upgrade but there is a good and
handy safety net in case of a mess.

The idea is to take a snapshot of the disk *partition*: it is like
making a backup of a partition and copy it back later whenever
required, with the added benefit of

- the backup is done in almost not time, it is *not* depending on the
  size of the disk,

- there is no actual copy of the blocks so it is very light on the
  io and space available,

- most importantly, even if the backup happens during a massive
  modification of the filesystem structure, the filesystem data
  structures are coherent.

It is not magic though, the snapshot itself is a partition and it
grows along when new data is added to the original partition to which
the snapshot is attached. Make sure the snapshot partition is big
enough to contain the updates to the original partition or suppress
the partition before it gets full.

Fake physical partitions
------------------------

Before showing how to setup a logical partition with lvm, let's first
present a cool trick to create *virtual physical partitions*: it is
not related to lvm per se but it ease experimentation with RAID or LVM
without mangling a real disk.

With *losetup*'s fake virtual partitions, it is possible to turn a
normal file into a disk image device, available in */dev*. Here we
create, with *dd*, a file of one gigabyte called *loop1.raw* in the
current directory, and make it available under */dev/loop1* ::

  ~# cd && sudo -s
  ~# dd if=/dev/zero of=loop1.raw bs=1M count=150

  ~# cat /proc/partitions 
  major minor  #blocks  name

   8        0  117220824 sda
   8        1     104391 sda1
   8        2   22964917 sda2

  ~# losetup /dev/loop1 loop1.raw

  ~# cat /proc/partitions 
   major minor  #blocks  name

   7        1     153600 loop1
   8        0  117220824 sda
   8        1     104391 sda1
   8        2   22964917 sda2

There was no partitions available called *loop1* and it appeared after
the *losetup command*. We will set up the partition to use with lvm in
the next paragraph.

LVM partition setup
-------------------

To represent LVM operations, there are three object you need to be
familiar with, and for each of these objects, there is the
corresponding shell commands for creating, listing and removing said
object.

`physical volume`: a physical partitions of one of the physical hard
   drive which has been allocated to lvm. Managed with *pvcreate*,
   *pvs* and *pvremove*.

`volume group`: aggregation of **physical volumes**. When I first read
   documentation about LVM, I thought the volume groups was meant to
   aggregate higher level logical paritions and data, but it is the
   other way round, they aggregate the real partitions. Managed with
   *vgcreate*, *vgs* and *vgremove*.

`logical volume`: volume is what you will use in the end with *mkfs*
   and *mount*. This is the device which look like the traditional
   partitions but with additional features like snapshots or resize on
   the fly. Managed with *lvcreate*, *lvs* and *lvgemove*.

Initialise the partition for use with lvm::

  ~# pvcreate /dev/loop1
    Physical volume "/dev/loop1" successfully created
  
A disk can only be added once to a volume group, multiple physical
disks compose a *volume group*::

  ~# vgcreate datadisks /dev/loop1
    Volume group "datadisks" successfully created

So far, the partitions available have not changed and the
*/dev/datadisks/website* partition does not exists::

  ~# lvs && cat /proc/partitions && ls /dev/datadisks/website
     # the partitions available have not changed yet and 
     # the /dev/datadisks/website partition does not exists (yet)

A logical volume (a partition) can now be created, it has a name, a
size, and is inside a group::

  ~# lvcreate -n website -L 100M datadisks
    Logical volume "website" created

  ~# lvs && cat /proc/partitions && ls /dev/datadisks/website

In the partitions, a new *dm* (I'll be it stands for *device mapper*)
entry is shown, the device is available contained in a directory named
after the volume group.

As usual, the partition must be formatted and mounted to be integrated
to the filesystem::

  ~# mkfs.ext4 /dev/datadisks/website
  ~# mkdir /mnt/website && mount /dev/datadisks/website /mnt/website


An upgrade plan in production
-----------------------------

Let's a compose a dummy three-tier website, that we will have to
upgrade, corrupt, rollback, etc::

   ~# touch /mnt/website/{database,index.html}
   ~# new_user_api () { echo "name:$1,age:$2" >> /mnt/website/database ; } 

With the adapted amount of marketing and public relation, the website
is put in production and made available to the public. Everyday,
torrents of new users line up to subscribe::

   ~# new_user_api alice 29
   ~# new_user_api bob 18

*Sparky the architect* have realised that the database schema must be
upgraded to include an *id* for each users. This is the current
database::

     ~# cat /mnt/website/database
     name:alice,age:29
     name:bob,age:18

It should end up look like this::

     id=001,name:alice,age:29
     id=002,name:bob,age:18

So the upgrade procedure is::

      ~# change_schema () {
          # you don't want to know ... 
          nl -n rz -w 5 /mnt/website/database \
             | sed 's/\t/,/; s/^/if:/' > /mnt/website/database.new
          mv /mnt/website/database{.new,} ; }

      ~# # Version 2 of the API
      ~# new_user_api () { 
            echo "id:$RANDOM,name:$1,age:$2" >> /mnt/website/database ; } 

Also, the website in production is not web2.0 enough, so a web
designer has done a great job beautifying a new prototype, which is
added to the upgrade procedure ::

  ~# touch /mnt/website/{social-caramels.js,ponies.js,eye-candy.css}

Filesystem transactions with lvm
--------------------------------

The system administrator tunes a transaction API and convince the
operator to use it the day of the upgrade. Before doing any change,
the operator must use the command *transaction*. If all is well after
a few days of testing, the command *commit* is used, else the operator
can use the *abort*.

The transaction functions are built on top of the LVM snapshot::

  ~# transaction () {
        lvcreate -s -n website_backup -L 25M  /dev/datadisks/website ; }

  ~# commit () { lvremove /dev/datadisks/website_backup ; }

  ~# abort () {
        mkdir /mnt/backup
        mount /dev/datadisks/website_backup /mnt/backup
        tar c -C /mnt/backup | tar x -C /mnt/website }

  ~# cleanup () {
        umount /mnt/backup/database
        rm -r  /mnt/backup/database
        lvremove /dev/datadisks/website_backup }

The upgrade procedure require the database to go read only, no new
users can be created. Comes the night of the upgrade, at dawn, the db
looks like::

  ~# cat /mnt/website/database
  if=001,name:alice,age:29
  if=002;name:bob,age:18

Ouuuch man! it is corrupted, there is no 'id' column instead it is
written 'if' everywhere now and we have no clue why. We need to go
back to the lab, figure out what happened... What do we do now with
this mess now? we need roll back so that the production site can
continue. Easy, here is the command::

   ~# abort

And just to make sure::

   ~# cat /mnt/website/database
   name:alice,age:29
   name:bob,age:18

Ok, the situation is similar as before the upgrade. The service can be
restored.

Upgrade which goes well
-----------------------

Three weeks later many many more users have been created::

  ~# cat /mnt/website/database
  name:alice,age:29
  name:bob,age:18
  name:robwilco,age:35
  name:duncanmacleod,age:539

and R&D has come up with a *complete* re-design of the upgrade
procedure: a snapshot and some *correct* database mangling
commands. Only the schema upgrade was modified::

   ~# change_schema () {
       # you don't want to know ... 
       nl -n rz -w 5 /mnt/website/database \
          | sed 's/\t/,/; s/^/if:/' > /mnt/website/database.new
       mv /mnt/website/database{.new,} ; }

At dawn, the database is correct, the snapshot safety net was
thankfully not used::

  ~# cat /mnt/website/database
  id:00001,name:alice,age:29
  id:00002,name:bob,age:18
  id:00003,name:robwilco,age:35
  id:00004,name:duncanmacleod,age:539

It is possible commit the transaction: remove the snapshot::

  ~# commit

and obviously, removing the snapshot does not impact the database::

  ~# cat /mnt/website/database
  id:00001,name:alice,age:29
  id:00002,name:bob,age:18
  id:00003,name:robwilco,age:35
  id:00004,name:duncanmacleod,age:539


We are done with this howto, to clean up after this exercice::

   ~# umount /mnt/website/
   ~# lvremove /dev/datadisks/website 
   ~# vgremove datadisks 
   ~# pvremove /dev/loop1
   ~# losetup -d /dev/loop1
   ~# rm hda1
